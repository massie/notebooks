{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "I wrote this notebook as a simple training exercise to better understand feedforward neural networks. The naming conventions in this code match with [Andrew Ng's](http://andrewng.com) free online [course in Machine Learning on Coursera](http://ml-class.org) (highly recommended). This neural network has a single hidden layer.\n",
    "\n",
    "Here's how the neural network is connected and equations for calculating the hypothesis, h_theta(x).\n",
    "![Feed Forward](files/feedforward.png)\n",
    "\n",
    "This neural network also implements [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) during training to determine the difference between the hypothesis and the training data in order to update the thetas, or weights, in the network.\n",
    "![Backpropagation](files/backpropagation.png)\n",
    "\n",
    "The example has a trivial training set with **X** equal to\n",
    "\n",
    "<table width=\"50%\">\n",
    "<tr><td>0</td><td>0</td></tr>\n",
    "<tr><td>0</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>0</td></tr>\n",
    "<tr><td>1</td><td>1</td></tr>\n",
    "</table>\n",
    "\n",
    "and the **y** vector used for this supervised learning matches the [exclusive or (XOR)](https://en.wikipedia.org/wiki/Exclusive_or) pattern.\n",
    "\n",
    "<table width=\"50%\">\n",
    "<tr><td>0</td></tr>\n",
    "<tr><td>1</td></tr>\n",
    "<tr><td>1</td></tr>\n",
    "<tr><td>0</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NumPy is the fundamental package for scientific computing with Python.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `theta_init` function is used to initialize the thetas (weights) in the network. It returns a random matrix with values in the range of [-epsilon, epsilon]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theta_init(in_size, out_size, epsilon = 0.12):\n",
    "    return np.random.rand(in_size + 1, out_size) * 2 * epsilon - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network uses a sigmoid activating function. The [sigmoid derivative](http://kawahara.ca/how-to-compute-the-derivative-of-a-sigmoid-function-fully-worked-example/) is used during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.divide(1.0, (1.0 + np.exp(-x)))\n",
    "def sigmoid_derivative(x):\n",
    "    return np.multiply(x, (1.0 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [mean squared error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) provides measure of the distance between the actual value and what is estimated by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(X):\n",
    "    return np.power(X, 2).mean(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn_train` function trains an artificial neural network with a single hidden layer. Each column in **X** is a feature and each row in **X** is a single training observation. The **y** value contains the classifications for each observation. For  multi-classification problems, **y** will have more than one column. After training, this function returns the calculated theta values (weights) that can be used for predictions.\n",
    "\n",
    "The training will end when the desired error or maximum iterations is reached whichever comes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nn_train(X, y, desired_error = 0.001, max_iterations = 100000, hidden_nodes = 5):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    input_nodes = X.shape[1]\n",
    "    output_nodes = y.shape[1]\n",
    "    \n",
    "    a1 = np.insert(X, 0, 1, axis=1)\n",
    "    theta1 = theta_init(input_nodes, hidden_nodes)\n",
    "    theta2 = theta_init(hidden_nodes, output_nodes)\n",
    "    \n",
    "    for x in range(0, max_iterations):\n",
    "        # Feedforward\n",
    "        a2 = np.insert(sigmoid(a1.dot(theta1)), 0, 1, axis=1)\n",
    "        a3 = sigmoid(a2.dot(theta2))\n",
    "        \n",
    "        # Calculate error using backpropagation\n",
    "        a3_delta = np.subtract(y, a3)\n",
    "        mse = mean_squared_error(a3_delta)\n",
    "        if mse <= desired_error:\n",
    "            print \"Achieved requested MSE %f at iteration %d\" % (mse, x)\n",
    "            break\n",
    "        a2_error = a3_delta.dot(theta2.T)\n",
    "        a2_delta = np.multiply(a2_error, sigmoid_derivative(a2))\n",
    "        \n",
    "        # Update thetas to reduce the error on the next iteration\n",
    "        theta2 += np.divide(a2.T.dot(a3_delta), m)\n",
    "        theta1 += np.delete(np.divide(a1.T.dot(a2_delta), m), 0, 1)\n",
    "        \n",
    "    return (theta1, theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn_predict` function takes the theta values calculated by `nn_train` to make predictions about the data in **X**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nn_predict(X, theta1, theta2):\n",
    "    a2 = sigmoid(np.insert(X, 0, 1, axis=1).dot(theta1))\n",
    "    return sigmoid(np.insert(a2, 0, 1, axis=1).dot(theta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "We start by plugging our data and classifications into our neural network which returns the weights we can use to make predictions with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved requested MSE 0.000999 at iteration 7575\n",
      "\n",
      "Trained weights for calculating the hidden layer from the input layer\n",
      "[[-2.5211517  -2.88490718 -0.30884227 -1.66214215 -5.05589632]\n",
      " [ 1.43593515  6.63503845  0.33924515  1.6955428   3.19690221]\n",
      " [ 1.80317086  6.54483663  0.97157529  0.75780739  3.33307961]]\n",
      "\n",
      "Trained weights for calculating from the hidden layer to the output layer\n",
      "[[ -2.00920204]\n",
      " [ -3.62687625]\n",
      " [ 10.48882278]\n",
      " [ -2.26565672]\n",
      " [ -3.4841157 ]\n",
      " [ -6.63951645]]\n"
     ]
    }
   ],
   "source": [
    "X = np.matrix('0 0; 0 1; 1 0; 1 1')\n",
    "y = np.matrix('0; 1; 1; 0')\n",
    "(theta1, theta2) = nn_train(X, y)\n",
    "print \"\\nTrained weights for calculating the hidden layer from the input layer\"\n",
    "print theta1\n",
    "print \"\\nTrained weights for calculating from the hidden layer to the output layer\"\n",
    "print theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained the neural network. We can make predictions for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for our test set is 0.468320\n",
      "[[ 0.          0.03181198 -0.03181198]\n",
      " [ 1.          0.97132027  0.02867973]\n",
      " [ 0.          0.03622773  0.96377227]\n",
      " [ 1.          0.97087072 -0.97087072]]\n"
     ]
    }
   ],
   "source": [
    "# Our test input doesn't match our training input 'X'\n",
    "X_test = np.matrix('1 1; 0 1; 0 0; 1 0')\n",
    "y_test = np.matrix('0; 1; 0; 1')\n",
    "y_calc = nn_predict(X_test, theta1, theta2)\n",
    "y_diff = np.subtract(y, y_calc)\n",
    "print \"The MSE for our test set is %f\" % (mean_squared_error(y_diff))\n",
    "print np.concatenate((y_test, y_calc, y_diff), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column one is the correct value, column two is the value predicted by this simple neural network, and the third column shows the different. The neural network correctly learning the XOR pattern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
