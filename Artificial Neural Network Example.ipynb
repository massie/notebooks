{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "I wrote this notebook as a simple training exercise using the simplest case to understand feedforward neural networks. The naming conventions in this code match with [Andrew Ng's](http://andrewng.com) free online [course in Machine Learning on Coursera](http://ml-class.org) (highly recommended). This example neural network has a single hidden layer.\n",
    "\n",
    "Here's how the neural network is connected and equations for calculating the hypothesis, $h_{\\theta}(x)$.\n",
    "![Feed Forward](feedforward.png)\n",
    "\n",
    "This neural network also implements [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) during training to determine the difference between the hypothesis and the training data in order to update the $\\theta{}s$, or weights, in the network.\n",
    "![Backpropagation](backpropagation.png)\n",
    "\n",
    "The example has a trivial training set with **X** equal to\n",
    "\n",
    "<table width=\"50%\">\n",
    "<tr><td>0</td><td>0</td></tr>\n",
    "<tr><td>0</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>0</td></tr>\n",
    "<tr><td>1</td><td>1</td></tr>\n",
    "</table>\n",
    "\n",
    "and the **y** vector used for this supervised learning matches the [exclusive or (XOR)](https://en.wikipedia.org/wiki/Exclusive_or) pattern.\n",
    "\n",
    "<table width=\"50%\">\n",
    "<tr><td>0</td></tr>\n",
    "<tr><td>1</td></tr>\n",
    "<tr><td>1</td></tr>\n",
    "<tr><td>0</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "*Note: the images above are from Andrew Ng's [Machine Learning Course](http://ml-class.org).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# NumPy is the fundamental package for scientific computing with Python.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `theta_init` function is used to initialize the thetas (weights) in the network. It returns a random matrix with values in the range of [-epsilon, epsilon]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_init(in_size, out_size, epsilon = 0.12):\n",
    "    return np.random.rand(in_size + 1, out_size) * 2 * epsilon - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network uses a sigmoid activating function. The [sigmoid derivative](http://kawahara.ca/how-to-compute-the-derivative-of-a-sigmoid-function-fully-worked-example/) is used during backpropagation.\n",
    "\n",
    "The sigmoid function ($\\sigma$) is: $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "The derivative of the sigmoid function is: $\\sigma(x) \\times (1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.divide(1.0, (1.0 + np.exp(-x)))\n",
    "def sigmoid_derivative(x):\n",
    "    return np.multiply(x, (1.0 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [mean squared error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) provides measure of the distance between the actual value and what is estimated by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(X):\n",
    "    return np.power(X, 2).mean(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn_train` function trains an artificial neural network with a single hidden layer. Each column in **X** is a feature and each row in **X** is a single training observation. The **y** value contains the classifications for each observation. For  multi-classification problems, **y** will have more than one column. After training, this function returns the calculated theta values (weights) that can be used for predictions.\n",
    "\n",
    "The training will end when the desired error or maximum iterations is reached whichever comes first. It returns the weights for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def nn_train(X, y, desired_error = 0.001, max_iterations = 100000, hidden_nodes = 5):\n",
    "\n",
    "    m = X.shape[0] # number of rows (samples)\n",
    "    input_nodes = X.shape[1] # number of columns\n",
    "    print(f\"Training data with {m} samples and {input_nodes} features\")\n",
    "    assert m == y.shape[0], f\"There are {m} samples in X but {y.shape[0]} y values\"\n",
    "    output_nodes = y.shape[1]\n",
    "\n",
    "    a1 = np.insert(X, 0, 1, axis=1) # add a_0^(1)\n",
    "    theta1 = theta_init(input_nodes, hidden_nodes)\n",
    "    theta2 = theta_init(hidden_nodes, output_nodes)\n",
    "    \n",
    "    for x in range(0, max_iterations):\n",
    "        # Feedforward\n",
    "        a2 = np.insert(sigmoid(a1.dot(theta1)), 0, 1, axis=1)\n",
    "        a3 = sigmoid(a2.dot(theta2))\n",
    "        \n",
    "        # Calculate error using backpropagation\n",
    "        a3_delta = np.subtract(y, a3)\n",
    "        mse = mean_squared_error(a3_delta)\n",
    "        if mse <= desired_error:\n",
    "            print (f\"Achieved requested MSE {mse} at iteration {x}\")\n",
    "            break\n",
    "        a2_error = a3_delta.dot(theta2.T)\n",
    "        a2_delta = np.multiply(a2_error, sigmoid_derivative(a2))\n",
    "        \n",
    "        # Update thetas to reduce the error on the next iteration\n",
    "        theta2 += np.divide(a2.T.dot(a3_delta), m)\n",
    "        theta1 += np.delete(np.divide(a1.T.dot(a2_delta), m), 0, 1)\n",
    "        \n",
    "    return (theta1, theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn_predict` function takes the theta values (weights) calculated by `nn_train` to make predictions about the data in **X**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def nn_predict(X, theta1, theta2):\n",
    "    a2 = sigmoid(np.insert(X, 0, 1, axis=1).dot(theta1))\n",
    "    return sigmoid(np.insert(a2, 0, 1, axis=1).dot(theta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "We start by plugging our data and classifications into our neural network which returns the weights we can use to make predictions with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.matrix('0 0; 0 1; 1 0; 1 1')\n",
    "y = np.matrix('0; 1; 1; 0')\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data with 4 samples and 2 features\n",
      "Achieved requested MSE 0.0009971347541874588 at iteration 3262\n",
      "\n",
      "Trained weights for calculating the hidden layer from the input layer\n",
      "[[ 0.05187587 -0.75564316  6.01590861  2.55036487 -0.61780587]\n",
      " [-0.05123144  0.85352135 -3.98152019 -6.28200152  0.79135377]\n",
      " [-0.18365105  0.857064   -3.97298513 -6.2828984   0.82203806]]\n",
      "\n",
      "Trained weights for calculating from the hidden layer to the output layer\n",
      "[[-0.91130206]\n",
      " [-0.20098316]\n",
      " [-2.29209456]\n",
      " [ 8.0253236 ]\n",
      " [-9.79200751]\n",
      " [-2.15564216]]\n"
     ]
    }
   ],
   "source": [
    "(theta1, theta2) = nn_train(X, y)\n",
    "print (\"\\nTrained weights for calculating the hidden layer from the input layer\")\n",
    "print (theta1)\n",
    "print (\"\\nTrained weights for calculating from the hidden layer to the output layer\")\n",
    "print (theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained the neural network. We can make predictions for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for our test set is 0.000997\n",
      "[[0.         0.03832304]\n",
      " [1.         0.97007261]\n",
      " [0.         0.02713896]\n",
      " [1.         0.97020551]]\n"
     ]
    }
   ],
   "source": [
    "# Our test input doesn't match our training input 'X'\n",
    "X_test = np.matrix('1 1; 0 1; 0 0; 1 0')\n",
    "y_test = np.matrix('0; 1; 0; 1')\n",
    "y_calc = nn_predict(X_test, theta1, theta2)\n",
    "y_diff = np.subtract(y_test, y_calc)\n",
    "print (\"The MSE for our test set is %f\" % (mean_squared_error(y_diff)))\n",
    "print (np.concatenate((y_test, y_calc), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column one is the correct value, column two is the value predicted by this simple neural network.\n",
    "\n",
    "The neural network correctly learned the XOR pattern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
